{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ActivationStoreParallel import ActivationsStore \n",
    "from sparse_transcoder import SparseTranscoder\n",
    "from transcoder_runner_parallel import language_model_transcoder_runner_parallel\n",
    "from dataclasses import dataclass   \n",
    "import transformer_lens   \n",
    "import torch   \n",
    "import wandb   \n",
    "from typing import Optional   \n",
    "import einops\n",
    "import plotly.express as px\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "\n",
    "class Config1():   \n",
    "        \n",
    "         # Data Generating Function (Model + Training Distibuion)   \n",
    "         model_name =  \"gpt2-small\"\n",
    "         hook_transcoder_in =   \"blocks.10.hook_resid_pre\"   \n",
    "         hook_point =   \"blocks.10.hook_resid_pre\"     \n",
    "         hook_transcoder_out =   \"blocks.10.attn.hook_q\"\n",
    "         target =   \"blocks.10.attn.hook_q\"\n",
    "         hook_point_layer = 10   \n",
    "         ln = 'blocks.10.ln1.hook_scale'   \n",
    "         d_in = 768   \n",
    "         d_out = 768 * 12   \n",
    "         n_head = 12   \n",
    "         d_head = 64   \n",
    "         dataset_path = \"Skylion007/openwebtext\" \n",
    "         is_dataset_tokenized=False   \n",
    "         layer = 10   \n",
    "         training = True   \n",
    "         attn_scores_normed = True \n",
    "            \n",
    "\n",
    "class QK_cfg():   \n",
    "         # Common settings   \n",
    "         model_name: str =  \"gpt2-small\"\n",
    "         hook_point: str =  \"blocks.10.hook_resid_pre\"   \n",
    "         ln: str = 'blocks.10.ln1.hook_scale'   \n",
    "         hook_point_layer: int = 10   \n",
    "         layer: int = 10   \n",
    "         d_in: int = 768   \n",
    "         d_out: int = 768   \n",
    "         n_head: int = 12   \n",
    "         d_head: int = 64   \n",
    "         dataset_path: str =  \"Skylion007/openwebtext\"   \n",
    "         is_dataset_tokenized: bool = False   \n",
    "         training: bool = True   \n",
    "         attn_scores_norm = True   \n",
    "\n",
    "            \n",
    "         # SAE Parameters   \n",
    "         d_hidden: int = 2400   \n",
    "         b_dec_init_method: str = \"mean\"    \n",
    "            \n",
    "         # Training Parameters   \n",
    "         lr: float = 1e-3\n",
    "         reg_coefficient: float = 15e-7 \n",
    "         lr_scheduler_name = \"cosineannealingwarmup\" \n",
    "         train_batch_size: int = 2048\n",
    "         context_size: int = 256   \n",
    "         lr_warm_up_steps: int = 5000   \n",
    "         norming_decoder_during_training = False\n",
    "            \n",
    "         # Activation Store Parameters   \n",
    "         n_batches_in_buffer: int = 128   \n",
    "         total_training_tokens: int = 20_000 * 10_000   \n",
    "         store_batch_size: int = 32   \n",
    "         use_cached_activations: bool = False   \n",
    "            \n",
    "         # Resampling protocol   \n",
    "         feature_sampling_method: str = 'none'   \n",
    "         feature_sampling_window: int = 1000   \n",
    "         feature_reinit_scale: float = 0.2   \n",
    "         resample_batches: int = 1028   \n",
    "         dead_feature_window: int = 50000   \n",
    "         dead_feature_threshold: float = 1e-6   \n",
    "            \n",
    "         # WANDB   \n",
    "         log_to_wandb: bool = True   \n",
    "         log_final_model_to_wandb: bool = False   \n",
    "         wandb_project: str =  \"Jul_24_test\"   \n",
    "         wandb_entity = \"kwyn390\"\n",
    "         wandb_log_frequency: int = 50   \n",
    "         entity: str =  \"kwyn390\" \n",
    "            \n",
    "         # Misc   \n",
    "         device: str =   \"cuda\"\n",
    "         eps: float = 1e-7   \n",
    "         seed: int = 42   \n",
    "         reshape_from_heads: bool = True   \n",
    "         n_checkpoints: int = 10   \n",
    "         checkpoint_path: str =   \"checkpoints\"   \n",
    "         dtype: torch.dtype = torch.float32   \n",
    "         run_name: str = \"qk_parallel\"\n",
    "            \n",
    "         # Query-specific settings   \n",
    "         hook_transcoder_in_q: str = \"blocks.10.hook_resid_pre\"  \n",
    "         hook_transcoder_out_q: str = \"blocks.10.attn.hook_q\"     \n",
    "         target_q: str = \"blocks.10.attn.hook_q\" \n",
    "         type_q: str = \"resid_to_queries\"    \n",
    "            \n",
    "         # Key-specific settings   \n",
    "         hook_transcoder_in_k: str =  \"blocks.10.hook_resid_pre\"  \n",
    "         hook_transcoder_out_k: str =   \"blocks.10.attn.hook_k\"\n",
    "         target_k: str =   \"blocks.10.attn.hook_k\"\n",
    "         type_k: str =   \"resid_to_keys\"     \n",
    "        \n",
    "qk_cfg = QK_cfg()   \n",
    "\n",
    "qk_cfg.run_name = f\"{qk_cfg.d_hidden}_{qk_cfg.lr}_{qk_cfg.reg_coefficient}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_data = torch.load()\n",
    "key_data = torch.load()\n",
    "\n",
    "query_transcoder = SparseTranscoder(qk_cfg, is_query = True)\n",
    "key_transcoder = SparseTranscoder(qk_cfg, is_query=False)\n",
    "\n",
    "query_transcoder.load_state_dict(query_data[\"state_dict\"])\n",
    "key_transcoder.load_state_dict(key_data[\"state_dict\"])\n",
    "\n",
    "query_transcoder.training = False\n",
    "key_transcoder.training = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_heads(tensor):\n",
    "    return einops.rearrange(tensor, \" ... n_head d_head -> ... (n_head d_head)\")\n",
    "\n",
    "def unflatten_heads(tensor, n_head):\n",
    "    return einops.rearrange(tensor, \" ... (n_head d_head) -> ... n_head d_head\", n_head=n_head)\n",
    "\n",
    "def flat_pattern_from_scores(scores, attn_scores_norm):\n",
    "    pattern = apply_causal_mask(scores / attn_scores_norm).log_softmax(-1)\n",
    "    flat_pattern = pattern.view((-1, pattern.shape[-1]))\n",
    "    return flat_pattern\n",
    "\n",
    "def apply_causal_mask(attn_scores):\n",
    "        mask = torch.triu(torch.ones(attn_scores.size(-2), attn_scores.size(-1)).cuda(), diagonal=1).bool()\n",
    "        # Apply the mask to attention scores, then return the masked scores\n",
    "        attn_scores.masked_fill_(mask, -1e9)\n",
    "        return attn_scores\n",
    "\n",
    "\n",
    "def compute_ground_truth(model, data, cfg, attn_scores_norm):\n",
    "    \"\"\"\n",
    "    Compute ground truth queries, keys, scores, and attention patterns (former 3 unscaled by attn_scores_norm)\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The main model being trained.\n",
    "        data (torch.Tensor): Input data tensor.\n",
    "        cfg (Config): Configuration object containing model parameters.\n",
    "        attn_scores_norm: Either 1 or 1/sqrt(d_head)\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contains true_queries, true_keys, true_scores, and true_patt tensors.\n",
    "    \"\"\"\n",
    "    true_queries = einops.einsum(model.W_Q[cfg.layer], data, \"n_head d_model d_head, ... d_model -> ... n_head d_head\") + model.b_Q[cfg.layer]\n",
    "    true_keys = einops.einsum(model.W_K[cfg.layer], data, \"n_head d_model d_head, ... d_model -> ... n_head d_head\") + model.b_K[cfg.layer]\n",
    "    true_scores = einops.einsum(true_queries, true_keys, \"... posn_q n_head d_head, ... posn_k n_head d_head -> ... n_head posn_q posn_k\")\n",
    "    true_patt_view = flat_pattern_from_scores(true_scores, attn_scores_norm)\n",
    "    return true_queries, true_keys, true_scores, true_patt_view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "### These have been re-jigged a bit to use the auto-encoder approach instead of transcoder\n",
    "@torch.no_grad()\n",
    "def get_transcoder_loss(batch_tokens, model, query_transcoder, key_transcoder, kl_loss = False):\n",
    "        \"\"\"\n",
    "        Loss when we patch in the attention calculated with the reconstructed keys and queries, no feature map involved\n",
    "        \"\"\"\n",
    "        query_hook = key_hook = f\"blocks.{query_transcoder.layer}.attn.q\"\n",
    "        key_hook = f\"blocks.{query_transcoder.layer}.attn.k\"\n",
    "        target_hook = f\"blocks.{query_transcoder.layer}.attn.pattern\"\n",
    "        query_cache = None\n",
    "        key_cache = None\n",
    "\n",
    "        def get_input_hook(activations, hook):\n",
    "            global comp_cache\n",
    "            comp_cache = activations\n",
    "            return activations\n",
    "        \n",
    "        def get_queries(activations, hook):\n",
    "            global query_cache\n",
    "            query_cache = activations\n",
    "            return activations\n",
    "        \n",
    "        def get_keys(activations, hook):\n",
    "            global query_cache\n",
    "            key_cache = activations\n",
    "            return activations\n",
    "\n",
    "        def replace_target_hook(activations, hook):\n",
    "            global query_cache, key_cache \n",
    "            \n",
    "            #Forward pass goes here \n",
    "            reconstr_queries = query_transcoder.forward(flatten_heads(query_cache))\n",
    "            reconstr_queries = unflatten_heads(reconstr_queries)\n",
    "            reconstr_keys = key_transcoder.forward(flatten_heads(key_cache))\n",
    "            reconstr_keys = unflatten_heads(reconstr_keys)\n",
    "            reconstr_scores = einops.einsum(reconstr_queries, reconstr_keys, \"batch posnQ n_head d_head, batch posnK n_head d_head -> batch n_head posnQ posnK\")/query_transcoder.d_head**0.5\n",
    "            reconstr_pattern = apply_causal_mask(reconstr_scores).softmax(-1)\n",
    "            return reconstr_pattern\n",
    "\n",
    "        if kl_loss:\n",
    "            return_type = \"logits\"\n",
    "        else:\n",
    "            return_type = \"loss\"\n",
    "            \n",
    "        res = model.run_with_hooks(\n",
    "            batch_tokens,\n",
    "            return_type=return_type,\n",
    "            fwd_hooks=[(get_queries, query_hook)] + [(get_keys, key_hook)] + [(target_hook, replace_target_hook)],\n",
    "        )\n",
    "           \n",
    "        return res\n",
    "    \n",
    "@torch.no_grad()\n",
    "def get_transcoder_loss(batch_tokens, model, query_transcoder, key_transcoder, feature_map, kl_loss = False):\n",
    "        \"\"\"\n",
    "        Loss when we patch in the attention calculated the expanded way using feature amp\n",
    "        \"\"\"\n",
    "        query_hook = key_hook = f\"blocks.{query_transcoder.layer}.attn.q\"\n",
    "        key_hook = f\"blocks.{query_transcoder.layer}.attn.k\"\n",
    "        target_hook = f\"blocks.{query_transcoder.layer}.attn.pattern\"\n",
    "        query_cache = None\n",
    "        key_cache = None\n",
    "        k_features = einops.rearrange(key_transcoder.W_dec, \"d_hidden (n_head d_head) -> d_hidden n_head d_head\", d_head = key_transcoder.d_head)\n",
    "\n",
    "\n",
    "        def get_input_hook(activations, hook):\n",
    "            global comp_cache\n",
    "            comp_cache = activations\n",
    "            return activations\n",
    "        \n",
    "        def get_queries(activations, hook):\n",
    "            global query_cache\n",
    "            query_cache = activations\n",
    "            return activations\n",
    "        \n",
    "        def get_keys(activations, hook):\n",
    "            global query_cache\n",
    "            key_cache = activations\n",
    "            return activations\n",
    "\n",
    "        def replace_target_hook(activations, hook):\n",
    "            global query_cache, key_cache, k_features\n",
    "            \n",
    "            #Forward pass goes here \n",
    "            feature_acts_Q = F.relu(einops.einsum((query_cache - query_transcoder.b_dec), query_transcoder.W_enc, \"... d_model, d_model d_hidden -> ... d_hidden\") + query_transcoder.b_enc)\n",
    "            feature_acts_K = F.relu(einops.einsum((key_cache - key_transcoder.b_dec), key_transcoder.W_enc, \"... d_model, d_model d_hidden -> ... d_hidden\") + key_transcoder.b_enc)\n",
    "            #given feature acts and map between features, compute attention contribution from feature-pairs\n",
    "            attn_contribution = einops.einsum(feature_acts_Q, feature_map, \"batch posnQ d_hidden_Q, d_hidden_Q d_hidden_K n_head -> batch posnQ d_hidden_K n_head\")\n",
    "            attn_contribution = einops.einsum(attn_contribution, feature_acts_K, \"batch posnQ d_hidden_K n_head, batch posnK d_hidden_K -> batch posnQ posnK n_head\")    \n",
    "            #compute attention contribution from key-features to query-biases\n",
    "            bias_reshape = einops.rearrange(query_transcoder.b_dec_out, \"(n_head d_head) -> n_head d_head\", n_head = query_transcoder.n_head)\n",
    "            bias_acts = einops.einsum(k_features, bias_reshape, \"d_hidden_K n_head d_head, n_head d_head -> n_head d_hidden_K\")\n",
    "            contr_from_bias = einops.einsum(bias_acts, feature_acts_K, \"n_head d_hidden_K, ... d_hidden_K -> ... n_head\").unsqueeze(1)\n",
    "            #pattern and loss\n",
    "            attn_scores_reconstr = (attn_contribution + contr_from_bias)/query_transcoder.d_head**0.5\n",
    "            attn_scores_reconstr = einops.rearrange(attn_scores_reconstr, \"batch posnQ posnK n_head -> batch n_head posnQ posnK\")\n",
    "            reconstr_pattern = apply_causal_mask(attn_scores_reconstr).softmax(-1)\n",
    "            return reconstr_pattern\n",
    "\n",
    "        if kl_loss:\n",
    "            return_type = \"logits\"\n",
    "        else:\n",
    "            return_type = \"loss\"\n",
    "            \n",
    "        res = model.run_with_hooks(\n",
    "            batch_tokens,\n",
    "            return_type=return_type,\n",
    "            fwd_hooks=[(get_queries, query_hook)] + [(get_keys, key_hook)] + [(target_hook, replace_target_hook)],\n",
    "        )\n",
    "           \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gotta load in your OV_transcoder and define config here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "### This is first stab at the OV patching run, maybe some bugs sorry!\n",
    "@torch.no_grad()\n",
    "def get_transcoder_loss(batch_tokens, model, ov_transcoder, key_transcoder, kl_loss = False):\n",
    "        \"\"\"\n",
    "        Loss when we patch in the attention calculated with the reconstructed keys and queries, no feature map involved\n",
    "        \"\"\"\n",
    "        resid_hook = f\"blocks.{ov_transcoder.layer}.resid_pre\"\n",
    "        ln_hook= f\"blocks.{ov_transcoder.layer}.ln1.hook_scale\" \n",
    "        key_hook = f\"blocks.{ov_transcoder.layer}.attn.k\"\n",
    "        pattern_hook = f\"blocks.{ov_transcoder.layer}.attn.pattern\"\n",
    "        target_hook = f\"blocks.{ov_transcoder.layer}.attn.out\"\n",
    "        \n",
    "\n",
    "        resid_cache = None\n",
    "        pattern_cache = None\n",
    "        key_acts = None\n",
    "\n",
    "        def get_resid_hook(activations, hook):\n",
    "            global resid_cache\n",
    "            resid_cache = activations\n",
    "            return activations\n",
    "        \n",
    "        def get_key_acts_hook(activations, hook):\n",
    "            global key_acts\n",
    "            key_acts = F.relu(einops.einsum((activations - key_transcoder.b_dec), key_transcoder.W_enc, \"... d_model, d_model d_hidden -> ... d_hidden\") + key_transcoder.b_enc)\n",
    "            return activations\n",
    "        \n",
    "        def get_ln_hook(activations, hook):\n",
    "            global resid_cache\n",
    "            #I have no fucking clue why the 1/3 power is needed here\n",
    "            resid_cache = resid_cache / activations ** (1/3)\n",
    "            return activations\n",
    "        \n",
    "        def get_pattern_hook(activations, hook):\n",
    "            global pattern_cache\n",
    "            pattern_cache = activations\n",
    "            return activations\n",
    "\n",
    "        def replace_target_hook(activations, hook):\n",
    "            global resid_cache, pattern_cache, key_acts\n",
    "            reconstr_out, _, _, _ = ov_transcoder(\n",
    "                resid_cache,\n",
    "                key_acts,\n",
    "                pattern_cache\n",
    "                )\n",
    "            \n",
    "            reconstr_attn_out = einops.rearrange(reconstr_out, \"... (n_head d_head) -> ... n_head d_head\", n_head = ov_transcoder.n_head)\n",
    "            \n",
    "            #Forward pass goes here \n",
    "            return reconstr_attn_out.sum(-2)\n",
    "\n",
    "        if kl_loss:\n",
    "            return_type = \"logits\"\n",
    "        else:\n",
    "            return_type = \"loss\"\n",
    "            \n",
    "        res = model.run_with_hooks(\n",
    "            batch_tokens,\n",
    "            return_type=return_type,\n",
    "            fwd_hooks=[(get_resid_hook, resid_hook)] + [(get_ln_hook, ln_hook)] + [(get_key_acts_hook, key_hook)] \n",
    "            + [(get_pattern_hook, pattern_hook)] + [(replace_target_hook, target_hook)]\n",
    "        )\n",
    "           \n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
